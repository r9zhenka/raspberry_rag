# Как это работает (простым языком)

## Что это вообще такое?

Это голосовой помощник для кафедры. Работает на маленьком компьютере Raspberry Pi.

Ты подходишь, нажимаешь кнопку (или говоришь "Окей кафедра"), задаёшь вопрос голосом — и он тебе голосом отвечает. Ответы берёт из документов кафедры (расписание, FAQ, инфа о преподавателях и т.д.).

Всё работает **без интернета**. Полностью локально.

## Как устроено внутри?

Есть 3 главных модуля. Они работают друг за другом как конвейер:

```
Ты говоришь в микрофон
        ↓
   [1. ASR]  — превращает голос в текст
        ↓
   [2. RAG]  — ищет ответ в документах
        ↓
   [3. TTS]  — превращает текст ответа обратно в голос
        ↓
Ты слышишь ответ из динамика
```

### 1. ASR (Automatic Speech Recognition) — голос в текст

Файл: `src/asr/recognizer.py`

Используется **Vosk** — это бесплатная офлайн-модель распознавания речи. Мы берём маленькую русскую модель (46 МБ), она живёт в памяти (~300 МБ) и слушает микрофон.

Ты говоришь "Какие предметы на первом курсе?" → Vosk выдаёт текст `"какие предметы на первом курсе"`.

### 2. RAG (Retrieval Augmented Generation) — поиск ответа

Файлы: `src/rag/` (5 файлов)

Это самая сложная часть. Работает в 3 шага:

**Шаг 1 — Индексация (делается заранее, один раз):**
- Берём документы кафедры (PDF, DOCX, TXT)
- Режем их на кусочки (чанки) по ~400 символов
- Каждый кусочек прогоняем через нейросеть **rubert-tiny2** — она превращает текст в набор из 312 чисел (вектор). Это как "отпечаток" смысла текста
- Все векторы сохраняем в **FAISS** — это быстрый поиск по векторам

**Шаг 2 — Поиск (при каждом вопросе):**
- Твой вопрос тоже превращаем в вектор через ту же нейросеть
- Ищем в FAISS самые похожие кусочки документов
- Это семантический поиск — он ищет по смыслу, а не по точным словам

**Шаг 3 — Ответ:**
- Берём найденный кусочек и оборачиваем: "По данным кафедры: ..."
- Или (если включен LLM режим) — прогоняем через маленькую языковую модель **Vikhr-1B**, которая формулирует красивый ответ

### 3. TTS (Text-to-Speech) — текст в голос

Файл: `src/tts/synthesizer.py`

Используется **Piper** — быстрый нейросетевой синтезатор речи. Русский женский голос "Ирина". Превращает текст ответа в звук и воспроизводит через динамик.

## Какие файлы за что отвечают

```
src/
├── main.py              — "мозг": запускает всё, связывает модули
├── config.py            — читает настройки из config/assistant.yaml
│
├── asr/                 — МОДУЛЬ 1: голос → текст
│   ├── recognizer.py    — распознавание речи (Vosk)
│   └── wake_word.py     — слушает "Окей кафедра"
│
├── rag/                 — МОДУЛЬ 2: поиск ответа
│   ├── document_loader.py — читает PDF/DOCX/TXT файлы
│   ├── embedder.py      — превращает текст в вектор (rubert-tiny2)
│   ├── indexer.py       — строит базу векторов (FAISS + SQLite)
│   ├── retriever.py     — ищет похожие кусочки по вектору
│   ├── generator.py     — формирует ответ из найденного
│   └── watcher.py       — следит за папкой с документами
│
├── tts/                 — МОДУЛЬ 3: текст → голос
│   └── synthesizer.py   — синтез речи (Piper)
│
├── audio/               — работа с микрофоном и динамиком
│   ├── recorder.py      — записывает голос с микрофона
│   └── player.py        — воспроизводит звук
│
├── hardware/            — железо
│   └── button.py        — кнопка (или Enter на клавиатуре)
│
└── utils/               — утилиты
    ├── memory.py        — контроль оперативной памяти
    └── sounds.py        — генерация звуковых сигналов
```

## Как запустить

### На обычном компьютере (для разработки)

Нужен Python 3.11+ и микрофон.

```bash
# 1. Клонируй репозиторий
git clone https://github.com/r9zhenka/raspberry_rag.git
cd raspberry_rag

# 2. Создай виртуальное окружение
python -m venv .venv

# На Windows:
.venv\Scripts\activate
# На Linux/Mac:
source .venv/bin/activate

# 3. Поставь зависимости
pip install -r requirements.txt

# 4. Скачай модели (~800 МБ)
# На Windows — скачай вручную:
#   - https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip
#     → распакуй в data/models/vosk-model-small-ru-0.22/
#   - Piper голос — см. README.md
#   - rubert-tiny2 — см. README.md
# На Linux:
bash scripts/download_models.sh

# 5. Проиндексируй документы
python -m src.rag.indexer

# 6. Запусти
python -m src.main
```

После запуска нажми **Enter** и задай вопрос голосом.

### На Raspberry Pi 5

```bash
git clone https://github.com/r9zhenka/raspberry_rag.git
cd raspberry_rag
bash scripts/install.sh
```

Всё. Скрипт сам поставит зависимости, скачает модели, проиндексирует документы и настроит автозапуск.

## Как добавить свои документы

Положи файлы (PDF, DOCX или TXT) в папку `data/documents/`. Система каждую минуту проверяет эту папку и автоматически индексирует новые файлы.

Или вручную:
```bash
python -m src.rag.indexer
```

## Настройки

Все настройки в одном файле: `config/assistant.yaml`

Что можно менять:
- `wake_word.phrase` — фраза активации (по умолчанию "окей кафедра")
- `wake_word.enabled` — включить/выключить wake word
- `hardware.button.gpio_pin` — номер GPIO-пина кнопки
- `rag.generator.mode` — `template` (быстро, просто) или `llm` (умнее, но медленнее)

## Сколько памяти ест

Raspberry Pi 5 имеет 4 ГБ оперативки. Вот сколько занимает каждый модуль:

| Что | Сколько RAM |
|-----|------------|
| Система (Linux) | ~500 МБ |
| Vosk (распознавание речи) | ~300 МБ |
| Piper (синтез речи) | ~150 МБ |
| FAISS (поиск) | ~10 МБ |
| rubert-tiny2 (embeddings) | ~150 МБ (загружается на момент, потом выгружается) |
| Vikhr-1B LLM (если включён) | ~1000 МБ (загружается на момент, потом выгружается) |

Фишка в том, что тяжёлые модели (embeddings и LLM) не сидят в памяти постоянно — загружаются только когда нужны, потом освобождают память.
